{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous post, I explored the [MovieLens dataset](https://grouplens.org/datasets/movielens/), a repository of over 26,000,000 movie ratings given to 45,000 movies by 270,000 users, and built a Collaborative-Filtering recommendation system that was able to predict ratings with an average absolute error of 0.66. \n",
    "\n",
    "I recommend that you read [the initial post](https://hexhamallstar.github.io/movie-rec.html) for a detailed exploration of the dataset and the details of implementing a Collaborative-Filtering recommendation system with NumPy.\n",
    "\n",
    "In this notebook, I will use PySpark's Alternating Least Squares (ALS) algorithm for model-based recommendations. \n",
    "\n",
    "## This notebook contains the following content:\n",
    "* [1. Model-based recommendations](#first-bullet)\n",
    "* [2. Working with PySpark](#second-bullet)\n",
    "* [3. Parameter search](#third-bullet)\n",
    "* [4. Conclusions](#fourth-bullet)\n",
    "\n",
    "## 1. Model-based recommendations <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "When building the Collaborative-Filtering model in my previous post, I ran into a number of limitations/issues:\n",
    "\n",
    "**Dataset size**\n",
    "* With 45,000 movies and 270,000 users, the non-sparse user-user similarity matrix would have required over 290GB of RAM to hold in memory.\n",
    "* Because of this, I had to use a much smaller sample of the data to build and test my model.\n",
    "\n",
    "**Similarity metric**\n",
    "* Cosine similarity was used, which calculates the angle between vectors. Missing ratings were treated as zero's in order to compute the similarity but this implies that the user did not like that movie (on our scale).\n",
    "* If 2 users have rated the exact same subset of the movies, and their ratings are multiples of one another (i.e. user1 = (1, 1, 0, 0, 0, 0, 0), user2 = (5, 5, 0, 0, 0, 0, 0) then the angle between the vectors is zero. These users will be deemed to be exactly the same despite them clearly not having the same preferences.\n",
    "\n",
    "For these reasons, I decided to try a model-based system. In this type of system, the user-item rating matrix is decomposed into a user-feature matrix and an item-feature matrix:\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-95QD5t9Lha4/Wd7uWnBZBeI/AAAAAAAADg4/xB4VnnxM0UgUp15lNmB3aHCXYGejpm4OACLcBGAs/s1600/matrix_factorization.png\">\n",
    "\n",
    "In this case, d would be the number of features that we 'learn' for each user and movie. We can use an optimization algorithm to learn the features that minimize the error in the matrix calculation shown above. The estimated user-item rating matrix is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{R} = U \\times V\n",
    "\\end{equation*}\n",
    "\n",
    "And the residual sum of squared (RSS) errors is:\n",
    "\\begin{equation*}\n",
    "RSS = \\sum (R - U \\times V)^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "And we therefore want to find the values of U and V that minimize the RSS. This looks very similar to the optimization of RSS for linear regression, however neither U or V are fixed making this a more complex problem (in linear regression we use the known input features and 'learn' the weights required for each one). \n",
    "\n",
    "The way this is implemented in Spark is to:\n",
    "* Fix the values of U, perform one step of gradient descent updating the values in V.\n",
    "* Fix the values of V, perform one step of gradient descent updating the values in U.\n",
    "* Repeat for a set number of iterations or until a convergence criteria is met.\n",
    "\n",
    "By alternately fixing one of the feature matrices this becomes a convex problem that is easily solved (and also easy to parallelize). This method is called Alternating Least Squares (ALS).\n",
    "\n",
    "## 2. Working with PySpark <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "PySpark is a convenient Python library that interfaces with Spark. For large datasets, a Spark-based system has advantages because:\n",
    "* Data imported into Spark RDD's/Dataframes is partitioned and can be easily worked upon in parallel.\n",
    "* Spark operations are lazily evaluated - defining operations creates a set of instructions that are only executed when some result is requested. Spark figures out how to distribute the computations to different cores and (often) does not need to materialize intermediate results which saves on time and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the SparkConfiguration and SparkContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "# if we wanted to change any configuration settings for this session only we would define them here\n",
    "conf = SparkConf() \n",
    "# create a SparkContext using the above configuration\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '51292'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.app.id', 'local-1525363027861'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '192.168.1.7'),\n",
       " ('spark.driver.memory', '14g'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command shows the current configuration settings\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important settings here are:\n",
    "* spark.rdd.compress - By default, compresses serialized RDD data. Takes some computation time but can give massive savings in memory used.\n",
    "* spark.master - Defines how many worker nodes we have. In this case 'local\\[*\\]' means we are running on a local machine and it will use the maximum number of threads available. On my CPU this is 8.\n",
    "* spark.driver.memory - Because we are performing matrix operations repeatedly with the rdd, we need a large amount of memory for the driver. I have assigned a maximum of 14GB out of my laptops 16GB total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import an SQL spark-session so that we can use dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "# import the ALS algorithm we will be using\n",
    "from pyspark.ml.recommendation import ALS\n",
    "# instantiate the SQL spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data, specifying that there is a header in the csv file \n",
    "# and that spark should auto-detect the variable types for each column\n",
    "data = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\")\\\n",
    "       .load(\"Movie Recommender/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema of the dataframe\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous post, the column names are userId, movieId, rating and timestamp. We will not be needing the timestamp column so we can safely drop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.drop('timestamp')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get spark to show us how many partitions it has split the dataframe up into. We can see below that there are 8 partitions (because my laptop's processor has 8 threads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also show the top N rows by using the take method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=1, movieId=110, rating=1.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit an ALS model. At first we will fit a model using the default parameters, specifying the required column names and the strategy for cold starts. Cold starts occur when we attempt to predict a rating for a user-movie pair but there were no ratings for this user/movie in the training set. There are 2 options for dealing with this:\n",
    "* \"NaN\" - return an empty variable. In production systems this may be useful because it would identify new users/items to the system and allow it to fall back onto another recommendation system such as a content-based system. In development however, this result prevents us from calculating a performance metric to evaluate the system.\n",
    "* \"drop\" - this option simply removes the row/column from the predictions and from the test set. Our result will therefore only contain valid numbers that can be used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the model, with the \"drop\" cold start strategy\n",
    "model = ALS(coldStartStrategy=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALS_4c41b64f738125c14df6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the column names for the required data\n",
    "model.setItemCol(\"movieId\")\\\n",
    "    .setUserCol(\"userId\")\\\n",
    "    .setRatingCol(\"rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model, we need to split our data such that we can evaluate the system on a set of data points that were not used to train the model. As in the previous post, I will perform a random split with 90% training data and 10% test data held out for evaluation. Dataframes in spark have a handy randomSplit method for doing just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into train and test sets with 90:10 proportions\n",
    "(train, test) = data.randomSplit([0.9, 0.1], seed=10)\n",
    "# since the train dataframe will be used many times, forcing spark to cache it could \n",
    "# reduce time taken, as we don't have to read from disk as much\n",
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the training set\n",
    "model = model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions by using the model to transform the test set\n",
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- prediction: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe that contains all of the data points in the test set (that weren't dropped due to cold start) with an extra column that contains the prediction. To evaluate the predictions, we compare them to the rating column.\n",
    "\n",
    "As in the previous post, we will use Mean Absolute Error as our measure of performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the regression evaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate evaluator, specifying the desired metric \"mae\" and the columns\n",
    "# that contain the predictions and the actual values\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\", predictionCol=\"prediction\", labelCol=\"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the output of our model\n",
    "mae = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Absolute Error is 0.635\n"
     ]
    }
   ],
   "source": [
    "print('The Mean Absolute Error is %.3f' % (mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have achieved an MAE of 0.635, which is lower than the MAE achieved by the collaborative-filtering method which achieved an MAE of 0.66. This was just using the default parameters, now we can try to improve the model by searching over a grid.\n",
    "\n",
    "## 3. Parameter Optimization <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "The parameters we will search over are:\n",
    "* Rank - The number of hidden features that we will use to describe the users/movies. This is equal to the value d in the diagram in section 1.\n",
    "* RegParam - The regularization parameter applied to the cost function. In actual fact the cost function is not the simple RSS function shown in section 1, but includes a regularizing term:\n",
    "\\begin{equation*}\n",
    "Cost(U) = \\sum (R - U \\times V)^{2} + \\lambda \\Vert U \\Vert ^{2}_{2}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Cost(V) = \\sum (R - U \\times V)^{2} + \\lambda \\Vert V \\Vert ^{2}_{2}\n",
    "\\end{equation*}\n",
    "\n",
    "If we have no regularization i.e. lambda is 0, we get the RSS result. If lambda is large, the values of the matrices U and V are forced to be small. This has the effect of preventing overfitting to the training set, which can improve performance if we were previously overfitting.\n",
    "\n",
    "To decide which combination of the above parameters we should use, we will section off a validation set from the training set. We will fit the model with each combination of parameters to the training set and choose the combination that has the lowest error on the validation set. This will then be used to transform the test set for final evaluation.\n",
    "\n",
    "If we were to use the test set instead of the validation set in this situation, information from the test set would be 'leaking' into our model and the model could potentially overfit to it and not generalize well to future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new ALS estimator\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "#define a grid for both parameters\n",
    "#this will test 9 different combinations of the 2 parameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [5, 10, 15]) \\\n",
    "    .addGrid(als.regParam, [1, 0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data with a ratio of 90% training, 10% validation\n",
    "# define the estimator and evaluator to use to determine the best model\n",
    "# also pass in the parameter grid to search over\n",
    "trainValSplit = TrainValidationSplit(estimator = als, estimatorParamMaps=paramGrid, \n",
    "                                    evaluator = RegressionEvaluator(metricName=\"mae\", predictionCol=\"prediction\", labelCol=\"rating\"), \n",
    "                                     trainRatio = 0.9, parallelism = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the training data\n",
    "model = trainValSplit.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the best model\n",
    "bestModel = model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there is currently no way in spark to see which combination of hyperparameters were used in the best model. We now use the best model to transform the test data and compute predictions & evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mean Absolute Error is 0.628\n"
     ]
    }
   ],
   "source": [
    "# transform test data using bestModel\n",
    "predictions = bestModel.transform(test)\n",
    "# evaluate the predictions\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print('The Mean Absolute Error is %.3f' % (mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a slight improvement over our previous MAE of 0.635! Further tuning could be carried out, as currently only a small range of values has been tested.\n",
    "\n",
    "## 4. Conclusions <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "To summarize:\n",
    "* We have built a recommender system that uses over 26 million data points to predict movie ratings for users, achieving an MAE of 0.628.\n",
    "* The approach used is highly scalable, and can be used with computational clusters using HDFS for much larger data files.\n",
    "* By using a parallel architecture we can make better use of hardware instead of using a pythonic serial calculation approach. This reduces runtimes for larger calculations.\n",
    "* We have improved upon our previous recommender system that used collaborative-filtering (the previous model achieved an MAE of 0.66). This could be a combination of having more training data available and the Matrix Factorization approach used.\n",
    "\n",
    "Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
